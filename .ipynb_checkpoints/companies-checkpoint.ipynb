{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "import unidecode\n",
    "import ftfy\n",
    "import re\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for reading in the articles from the dataframe\n",
    "import ast\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the cleaned data / clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 730\n",
      "Number of Total Sentences: 708281\n",
      "Number of Unique Companies: 2592\n",
      "Number of Unique Ceos: 1291\n"
     ]
    }
   ],
   "source": [
    "# load in the corpus, stop_words, and cleaned companies\n",
    "sentences = pd.read_csv(\"all_sentences.csv\")\n",
    "sentences = [x[0] for x in sentences.values if x is not None]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "companies = pd.read_csv(\"all/companies.csv\", encoding = \"latin-1\", header = -1)[0].tolist()\n",
    "# remove duplicate sentences\n",
    "companies = list(set(companies))\n",
    "# articles\n",
    "articles = pd.read_csv(\"articles_sentences.csv\")\n",
    "list_of_articles = [ast.literal_eval(x) for x in articles['Article'].values]\n",
    "\n",
    "# load in CEOs for negative samples?\n",
    "ceos = pd.read_csv(\"all/ceo.csv\", encoding = \"latin-1\", header = -1).replace(np.nan, '', regex = True)\n",
    "ceos[2] = (ceos[0].str.rstrip() + \" \" + ceos[1].str.rstrip()).str.rstrip()\n",
    "cleaned_ceos = list(set(ceos[2].tolist()))\n",
    "\n",
    "print(\"Number of Articles: {}\".format(len(list_of_articles)))\n",
    "print(\"Number of Total Sentences: {}\".format(len(sentences)))\n",
    "print(\"Number of Unique Companies: {}\".format(len(companies)))\n",
    "print(\"Number of Unique Ceos: {}\".format(len(cleaned_ceos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Feature Vectors\n",
    "\n",
    "Features include:\n",
    "* Number of words in the sentence\n",
    "* Number of characters in the sentence\n",
    "* Number of characters in the target word/phrase\n",
    "* Number of capitals in the target word\n",
    "* Number of capitals in the sentence\n",
    "* Starting position of the word in the sentence\n",
    "* Number of words in the article\n",
    "* Number of sentences in the article\n",
    "* Number of times the word appears in the article\n",
    "* 1 if the word contains a keyword\n",
    "* number of words in the potential company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set(['Inc', 'inc', 'Corp', 'corp', 'corporation', 'Co', 'company', 'Company', 'Group', 'Ltd', 'ltd', 'Capital', 'capital',\n",
    "               'management', 'Management', 'Financial', 'financial', 'consulting', 'Consulting', 'Depot'])\n",
    "\n",
    "def build_feature_vector(word_tuple):\n",
    "    '''\n",
    "    word_tuple: (word, sentence_index, article_index)\n",
    "    '''\n",
    "    word = word_tuple[0]\n",
    "    sentence_index = word_tuple[1]\n",
    "    article_index = word_tuple[2]\n",
    "    \n",
    "    sentence = list_of_articles[article_index][sentence_index]\n",
    "    list_of_sentences = list_of_articles[article_index]\n",
    "    # number of words in the sentence\n",
    "    number_of_words = len(sentence.split(' '))\n",
    "    # number of characters in the sentence\n",
    "    number_of_chars_sentence = len(sentence)\n",
    "    # number of characters in the target\n",
    "    number_of_chars_candidate = len(word)\n",
    "    # number of capitals in the target word\n",
    "    number_capitals_word = sum(1 for x in word if x.isupper())\n",
    "    # starting position in the sentence\n",
    "    starting_index = sentence.find(word)\n",
    "    # number of capitals sentence\n",
    "    number_capitals_sentence = sum(1 for x in sentence if x.isupper())\n",
    "    # number of words in the article\n",
    "    sum_article = 0\n",
    "    for x in list_of_sentences:\n",
    "        sum_article += len(x.split(' '))\n",
    "    number_of_words_article = sum_article\n",
    "    # number of sentences in the article\n",
    "    number_of_sentences_article = len(list_of_sentences)\n",
    "    # number of times the word appears in the article\n",
    "    total_string = ' '.join(list_of_sentences)\n",
    "    appearance_count = total_string.count(word)\n",
    "    # 1 if the word contains a keyword\n",
    "    keyword_appear = 0\n",
    "    for subword in word.split(' '):\n",
    "        if subword in keywords:\n",
    "            keyword_appear = 1\n",
    "            break\n",
    "    \n",
    "    # number of words in the target word\n",
    "    candidate_word_count = len(word.split(' '))\n",
    "    \n",
    "    vector = [number_of_words, number_of_chars_sentence, number_of_chars_candidate,\n",
    "             number_capitals_word, starting_index, number_capitals_sentence, number_of_words_article,\n",
    "             number_of_sentences_article, appearance_count, keyword_appear, candidate_word_count]\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify training data from corpus\n",
    "\n",
    "Here we use company names as positive samples, and CEO names as negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Completed: 0%\n",
      "Percentage Completed: 10%\n",
      "Percentage Completed: 19%\n",
      "Percentage Completed: 29%\n",
      "Percentage Completed: 38%\n",
      "Percentage Completed: 48%\n",
      "Percentage Completed: 58%\n",
      "Percentage Completed: 67%\n",
      "Percentage Completed: 77%\n",
      "Percentage Completed: 86%\n",
      "Percentage Completed: 96%\n",
      "Time to complete: 395.7308769226074\n"
     ]
    }
   ],
   "source": [
    "# find positive and negative matches to train against\n",
    "pos_matches = []\n",
    "neg_matches = []\n",
    "start_time = time.time()\n",
    "# find matches with word, sentence_index, article_index\n",
    "for article_index, article in enumerate(list_of_articles):\n",
    "    if article_index % 70 == 0:\n",
    "        print(\"Percentage Completed: {0:.0%}\".format(article_index / len(list_of_articles)))\n",
    "    for sentence_index, sentence in enumerate(article):\n",
    "        for company in companies:\n",
    "            if company in sentence:\n",
    "                pos_matches.append((company, sentence_index, article_index))\n",
    "        for ceo in cleaned_ceos:\n",
    "            if ceo in sentence:\n",
    "                neg_matches.append((ceo, sentence_index, article_index))\n",
    "                \n",
    "print(\"Time to complete: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature vectors for positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Positive samples...\n",
      "Number of Positive samples: 191661\n",
      "Time to finish positive samples: 335.9750978946686\n",
      "Starting Negative samples...\n",
      "Number of Negative samples: 38739\n",
      "Time to finish negative samples: 69.47800898551941\n"
     ]
    }
   ],
   "source": [
    "# Add Labels\n",
    "print(\"Starting Positive samples...\")\n",
    "start_time = time.time()\n",
    "pos_data = list(map(lambda x: build_feature_vector(x), pos_matches))\n",
    "for vector in pos_data:\n",
    "    vector.append(1)\n",
    "print(\"Number of Positive samples: {}\".format(len(pos_data)))\n",
    "print(\"Time to finish positive samples: {}\".format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting Negative samples...\")\n",
    "neg_data = list(map(lambda x: build_feature_vector(x), neg_matches))\n",
    "for vector in neg_data:\n",
    "    vector.append(0)\n",
    "print(\"Number of Negative samples: {}\".format(len(neg_data)))\n",
    "print(\"Time to finish negative samples: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Split the data into training and testing data, tune parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 25658    304]\n",
      " [   285 128121]]\n",
      "Accuracy: 0.996184442371476\n",
      "Precision: 0.9882905785378631\n",
      "Recall: 0.9890143776741317\n"
     ]
    }
   ],
   "source": [
    "# combine the data\n",
    "combined_data = pd.DataFrame(pos_data + neg_data)\n",
    "x_values = combined_data.drop(11, axis = 1)\n",
    "y_values = combined_data[11]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.33)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 30)\n",
    "rfc.fit(x_train, y_train)\n",
    "rf_fit_values = rfc.predict(x_train)\n",
    "confus = confusion_matrix(y_train, rf_fit_values)\n",
    "print(confus)\n",
    "accuracy = (confus[0,0] + confus[1,1]) / sum(sum(confus))\n",
    "precision = (confus[0,0] / (confus[0,0] + confus[0,1]))\n",
    "recall = (confus[0,0] / (confus[0,0] + confus[1,0]))\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7773  5004]\n",
      " [ 2815 60440]]\n",
      "Accuracy: 0.8971617213804713\n",
      "Precision: 0.6083587696642404\n",
      "Recall: 0.7341329807329052\n"
     ]
    }
   ],
   "source": [
    "rf_fit_values = rfc.predict(x_test)\n",
    "confus = confusion_matrix(y_test, rf_fit_values)\n",
    "print(confus)\n",
    "accuracy = (confus[0,0] + confus[1,1]) / sum(sum(confus))\n",
    "precision = (confus[0,0] / (confus[0,0] + confus[0,1]))\n",
    "recall = (confus[0,0] / (confus[0,0] + confus[1,0]))\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify potential companies\n",
    "\n",
    "We use a regex that follows this idea:\n",
    "\n",
    "* This Matches The Regex\n",
    "\n",
    "It gathers all the words that have capital letters to start in a sequence\n",
    "\n",
    "For instance:\n",
    "\n",
    "* \"This would only match the first word\" would return \"This\" as a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Completed: 0%\n",
      "Percentage Completed: 10%\n",
      "Percentage Completed: 19%\n",
      "Percentage Completed: 29%\n",
      "Percentage Completed: 38%\n",
      "Percentage Completed: 48%\n",
      "Percentage Completed: 58%\n",
      "Percentage Completed: 67%\n",
      "Percentage Completed: 77%\n",
      "Percentage Completed: 86%\n",
      "Percentage Completed: 96%\n",
      "Number of potential matches: 15301\n",
      "Time to complete: 6.928769826889038\n"
     ]
    }
   ],
   "source": [
    "# identify which sentences may have a company in it\n",
    "def identify_potential_sentence(sentence):\n",
    "    matches = re.findall(r\"(?:(?:[A-Z]+[a-z]*) ?)+\", sentence)\n",
    "    if matches:\n",
    "        return matches\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# removes the stop words from a found match\n",
    "def remove_stop_words(word):\n",
    "    cleaned_word = ' '.join([x for x in word.split(' ') if x.lower() not in stop_words]).rstrip()\n",
    "    return cleaned_word\n",
    "\n",
    "# find positive and negative matches to train against\n",
    "potential_matches = []\n",
    "start_time = time.time()\n",
    "# find matches with word, sentence_index, article_index\n",
    "# only select matches that are their own word\n",
    "for article_index, article in enumerate(list_of_articles):\n",
    "    if article_index % 70 == 0:\n",
    "        print(\"Percentage Completed: {0:.0%}\".format(article_index / len(list_of_articles)))\n",
    "    for sentence_index, sentence in enumerate(article):\n",
    "        matches = identify_potential_sentence(sentence) \n",
    "        if matches:\n",
    "            filtered_matches = []\n",
    "            # filter out the matches with annoying APBloomberg or AP stuff in it\n",
    "            for match in matches:\n",
    "                if \" \" + match + \" \" in sentence:\n",
    "                    filtered_matches.append(match)\n",
    "                elif sentence[:len(match)+1 == match + \" \"]:\n",
    "                    filtered_matches.append(match)                 \n",
    "                elif sentence[-len(match)+1:] == \" \" + match:\n",
    "                    filtered_matches.append(match)\n",
    "            cleaned_matches = [remove_stop_words(x) for x in filtered_matches]\n",
    "            remove_empty_words = [x for x in cleaned_matches if x != '']\n",
    "            for match in remove_empty_words:\n",
    "                potential_matches.append((match, sentence_index, article_index))\n",
    "\n",
    "potential_matches = list(set(potential_matches))\n",
    "print(\"Number of potential matches: {}\".format(len(potential_matches)))\n",
    "print(\"Time to complete: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build feature vectors for the potential samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification of potential samples...\n",
      "Number of samples: 15301\n",
      "Time to finish: 27.088132858276367\n"
     ]
    }
   ],
   "source": [
    "# Add Labels\n",
    "print(\"Starting classification of potential samples...\")\n",
    "start_time = time.time()\n",
    "new_data = list(map(lambda x: build_feature_vector(x), potential_matches))\n",
    "print(\"Number of samples: {}\".format(len(new_data)))\n",
    "print(\"Time to finish: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the potential samples based on the model and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit_values = rfc.predict(new_data)\n",
    "classified_companies = []\n",
    "for index, classification in enumerate(rf_fit_values):\n",
    "    if classification == 1:\n",
    "        classified_companies.append(potential_matches[index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_companies = set(classified_companies)\n",
    "output = set(list(found_companies) + list(companies))\n",
    "pd.Series(list(output)).to_csv(\"found_companies.csv\", header = False, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
