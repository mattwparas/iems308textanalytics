{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import nltk\n",
    "import time\n",
    "import unidecode\n",
    "import ftfy\n",
    "import re\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data, clean the training CEO data set\n",
    "\n",
    "We clean the CEO data by stripping away any of the extra white space, and combining first and last names into one columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 730\n",
      "Number of Total Sentences: 708281\n",
      "Number of Unique Companies: 2592\n",
      "Number of Unique Ceos: 1291\n"
     ]
    }
   ],
   "source": [
    "# load in the corpus, stop_words, and cleaned companies\n",
    "sentences = pd.read_csv(\"all_sentences.csv\")\n",
    "sentences = [x[0] for x in sentences.values if x is not None]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "companies = pd.read_csv(\"all/companies.csv\", encoding = \"latin-1\", header = -1)[0].tolist()\n",
    "# remove duplicate sentences\n",
    "companies = list(set(companies))\n",
    "# articles\n",
    "articles = pd.read_csv(\"articles_sentences.csv\")\n",
    "list_of_articles = [ast.literal_eval(x) for x in articles['Article'].values]\n",
    "\n",
    "# load in CEOs for negative samples?\n",
    "ceos = pd.read_csv(\"all/ceo.csv\", encoding = \"latin-1\", header = -1).replace(np.nan, '', regex = True)\n",
    "ceos[2] = (ceos[0].str.rstrip() + \" \" + ceos[1].str.rstrip()).str.rstrip()\n",
    "cleaned_ceos = list(set(ceos[2].tolist()))\n",
    "\n",
    "print(\"Number of Articles: {}\".format(len(list_of_articles)))\n",
    "print(\"Number of Total Sentences: {}\".format(len(sentences)))\n",
    "print(\"Number of Unique Companies: {}\".format(len(companies)))\n",
    "print(\"Number of Unique Ceos: {}\".format(len(cleaned_ceos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the feature vector\n",
    "* number of words in the sentence\n",
    "* number of characters in the sentence\n",
    "* number of characters in the potential CEO\n",
    "* number of capitals in the potential CEO\n",
    "* number of capitals in the sentence\n",
    "* starting position of the word in the sentence\n",
    "* number of words in the article\n",
    "* number of sentences in the article\n",
    "* number of times the word appears in the article\n",
    "* 1 if the article contains a keyword from the first set of keywords\n",
    "* length of the first and last name\n",
    "* number of times the first and last time appear in the article\n",
    "* number of times the second set of keywords appear in the sentence and article\n",
    "* number of words in the potential CEO name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = set(['Inc', 'inc', 'Corp', 'corp', 'corporation', 'Co', 'company', 'Company', 'Group', 'Ltd', 'ltd', 'Capital', 'capital',\n",
    "               'management', 'Management', 'Financial', 'financial', 'consulting', 'Consulting', 'Depot'])\n",
    "\n",
    "keywords2 = set(['CEO', 'Chief', 'Executive', 'executive', 'ceo', 'Officer', 'Company', 'company'])\n",
    "\n",
    "def build_feature_vector(word_tuple):\n",
    "    '''\n",
    "    word_tuple: (word, sentence_index, article_index)\n",
    "    '''\n",
    "    word = word_tuple[0]\n",
    "    sentence_index = word_tuple[1]\n",
    "    article_index = word_tuple[2]\n",
    "    \n",
    "    sentence = list_of_articles[article_index][sentence_index]\n",
    "    list_of_sentences = list_of_articles[article_index]\n",
    "    # number of words in the sentence\n",
    "    number_of_words = len(sentence.split(' '))\n",
    "    # number of characters in the sentence\n",
    "    number_of_chars_sentence = len(sentence)\n",
    "    # number of characters in the target\n",
    "    number_of_chars_candidate = len(word)\n",
    "    # number of capitals in the target word\n",
    "    number_capitals_word = sum(1 for x in word if x.isupper())\n",
    "    # starting position in the sentence\n",
    "    starting_index = sentence.find(word)\n",
    "    # number of capitals sentence\n",
    "    number_capitals_sentence = sum(1 for x in sentence if x.isupper())\n",
    "    # number of words in the article\n",
    "    sum_article = 0\n",
    "    for x in list_of_sentences:\n",
    "        sum_article += len(x.split(' '))\n",
    "    number_of_words_article = sum_article\n",
    "    # number of sentences in the article\n",
    "    number_of_sentences_article = len(list_of_sentences)\n",
    "    # number of times the word appears in the article\n",
    "    total_string = ' '.join(list_of_sentences)\n",
    "    appearance_count = total_string.count(word)\n",
    "    # 1 if the word contains a keyword\n",
    "    keyword_appear = 0\n",
    "    for subword in word.split(' '):\n",
    "        if subword in keywords:\n",
    "            keyword_appear = 1\n",
    "            break\n",
    "    \n",
    "    # len of first and last name\n",
    "    split_words = word.split(' ')\n",
    "    first_name = split_words[0]\n",
    "    \n",
    "    if len(split_words) > 1:\n",
    "        last_name = split_words[1]\n",
    "    else:\n",
    "        last_name = ''\n",
    "    \n",
    "    # first_name appearance count\n",
    "    first_name_count = total_string.count(\" \" + first_name + \" \")\n",
    "    \n",
    "    if last_name == '':\n",
    "        last_name_count = 0\n",
    "    else:\n",
    "        last_name_count = total_string.count(\" \" + last_name + \" \")\n",
    "    \n",
    "    sentence_keyword = 0\n",
    "    article_keyword = 0\n",
    "    for keyword in keywords2:\n",
    "        sentence_keyword += sentence.count(keyword)\n",
    "        article_keyword += total_string.count(keyword)\n",
    "    \n",
    "    # number of words in the target word\n",
    "    candidate_word_count = len(word.split(' '))\n",
    "    \n",
    "    vector = [number_of_words, number_of_chars_sentence, number_of_chars_candidate,\n",
    "             number_capitals_word, starting_index, number_capitals_sentence, number_of_words_article,\n",
    "             number_of_sentences_article, appearance_count, keyword_appear, candidate_word_count, first_name_count, last_name_count,\n",
    "             sentence_keyword, article_keyword]\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify candidate names \n",
    "\n",
    "Here we identify candidate names by selecting patterns of two words with capital letters:\n",
    "* Test Word\n",
    "\n",
    "would satisfy the candidate\n",
    "\n",
    "We then clean the word by removing stop words and also filtering out any potential words if they are in the bad keyword list. This was refined after visually inspecing potential keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Completed: 0%\n",
      "Percentage Completed: 10%\n",
      "Percentage Completed: 19%\n",
      "Percentage Completed: 29%\n",
      "Percentage Completed: 38%\n",
      "Percentage Completed: 48%\n",
      "Percentage Completed: 58%\n",
      "Percentage Completed: 67%\n",
      "Percentage Completed: 77%\n",
      "Percentage Completed: 86%\n",
      "Percentage Completed: 96%\n",
      "Time to complete: 4.2578887939453125\n"
     ]
    }
   ],
   "source": [
    "# filter keywords\n",
    "bad_keywords = set(['Inc', 'inc', 'Corp', 'corp', 'corporation', 'Co', 'company', 'Company', 'Group', 'Ltd', 'ltd', 'Capital', 'capital',\n",
    "               'management', 'Management', 'Financial', 'financial', 'consulting', 'Consulting', 'Depot', 'China', 'USA', 'Asia', 'North America',\n",
    "                   'Administration', 'Department', 'Business', 'Industry', 'Institute', 'United', 'States', 'Asia', 'Europe', 'New', 'York', 'Chicago',\n",
    "                   'Houston', 'Los', 'Angeles', 'National', 'President', 'Representative', 'House', 'Representatives', 'Senator', 'CFO', \n",
    "                   'Mojave', 'Desert', 'Olympics', 'Obama', 'Secretary', 'General', 'Inspector', 'Advisor', 'Economic', 'Atlantic', 'Gulf', 'Pacific', 'Ocean', \n",
    "                   'Finance', 'Wall Street', 'Wall', 'Street', 'Federal', 'Affordable', 'Republicans', 'Democrats', 'Congressional', 'Aviation', 'Internet', 'Hong', \n",
    "                   'Kong', 'Beijing', 'Africa', 'Russia', 'Government', 'Research', 'Council', 'Public', 'Service', 'Mobility', 'Bitcoin', 'Economy', 'Commodity',\n",
    "                   'Prices', 'Presentation', 'Citi', 'Navy', 'Jewish', 'Muslim', 'Journal', 'British', 'Zillow', 'Egypt', 'Congo', 'Kitchen', 'Thrift', 'Savings', \n",
    "                   'Director', 'Iraq', 'Iran', 'War', 'Saudi', 'Arabia', 'Oil', 'Turkey', 'Greece', 'Investment', 'Production', 'User', 'Experience', 'Western',\n",
    "                   'Eastern', 'Bank', 'Access', 'Debt', 'Growth', 'Resources', 'Brazil', 'Mexico', 'Canada', 'Canadian', 'American', 'English', 'Chinese', \n",
    "                   'Dangerous'])\n",
    "\n",
    "\n",
    "# identify which sentences may have a company in it\n",
    "def identify_potential_sentence(sentence):\n",
    "    matches = re.findall(r\"(?=([A-Z][a-z]+ [A-Z][a-z]+))\", sentence)\n",
    "    if matches:\n",
    "        return matches\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# removes the stop words from a found match\n",
    "def remove_stop_words(word):\n",
    "    cleaned_word = ' '.join([x for x in word.split(' ') if x.lower() not in stop_words]).rstrip()\n",
    "    return cleaned_word\n",
    "\n",
    "# find positive and negative matches to train against\n",
    "potential_matches = []\n",
    "start_time = time.time()\n",
    "# find matches with word, sentence_index, article_index\n",
    "# only select matches that are their own word\n",
    "for article_index, article in enumerate(list_of_articles):\n",
    "    if article_index % 70 == 0:\n",
    "        print(\"Percentage Completed: {0:.0%}\".format(article_index / len(list_of_articles)))\n",
    "    for sentence_index, sentence in enumerate(article):\n",
    "        matches = identify_potential_sentence(sentence) \n",
    "        if matches:\n",
    "            filtered_matches = []\n",
    "            # filter out the matches with annoying APBloomberg or AP stuff in it\n",
    "            for match in matches:\n",
    "                if \" \" + match + \" \" in sentence:\n",
    "                    filtered_matches.append(match)\n",
    "                elif sentence[:len(match)+1 == match + \" \"]:\n",
    "                    filtered_matches.append(match)                 \n",
    "                elif sentence[-len(match)+1:] == \" \" + match:\n",
    "                    filtered_matches.append(match)\n",
    "            \n",
    "            filtered_2 = []\n",
    "            for match in filtered_matches:\n",
    "                split_word = match.split(' ')\n",
    "                if split_word[0] not in bad_keywords and split_word[1] not in bad_keywords:\n",
    "                    filtered_2.append(match)\n",
    "                    \n",
    "            cleaned_matches = [remove_stop_words(x) for x in filtered_2]\n",
    "            remove_empty_words = [x for x in cleaned_matches if x != '']\n",
    "            for match in remove_empty_words:\n",
    "                potential_matches.append((match, sentence_index, article_index))\n",
    "\n",
    "potential_matches = list(set(potential_matches))\n",
    "print(\"Time to complete: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build positive and negative matches from the corpus from the training set\n",
    "\n",
    "Here, we use positive samples as the training CEO data set, and the negative as the training company data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage Completed: 0%\n",
      "Percentage Completed: 10%\n",
      "Percentage Completed: 19%\n",
      "Percentage Completed: 29%\n",
      "Percentage Completed: 38%\n",
      "Percentage Completed: 48%\n",
      "Percentage Completed: 58%\n",
      "Percentage Completed: 67%\n",
      "Percentage Completed: 77%\n",
      "Percentage Completed: 86%\n",
      "Percentage Completed: 96%\n",
      "Time to complete: 413.20392179489136\n"
     ]
    }
   ],
   "source": [
    "# find positive and negative matches to train against\n",
    "pos_matches = []\n",
    "neg_matches = []\n",
    "start_time = time.time()\n",
    "# find matches with word, sentence_index, article_index\n",
    "for article_index, article in enumerate(list_of_articles):\n",
    "    if article_index % 70 == 0:\n",
    "        print(\"Percentage Completed: {0:.0%}\".format(article_index / len(list_of_articles)))\n",
    "    for sentence_index, sentence in enumerate(article):\n",
    "        for company in companies:\n",
    "            if company in sentence:\n",
    "                neg_matches.append((company, sentence_index, article_index))\n",
    "        for ceo in cleaned_ceos:\n",
    "            if ceo in sentence:\n",
    "                pos_matches.append((ceo, sentence_index, article_index))\n",
    "                \n",
    "print(\"Time to complete: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature vectors for positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Positive samples...\n",
      "Number of Positive samples: 38739\n",
      "Time to finish positive samples: 109.19758582115173\n",
      "Starting Negative samples...\n",
      "Number of Negative samples: 191661\n",
      "Time to finish negative samples: 557.5668721199036\n"
     ]
    }
   ],
   "source": [
    "# Add Labels\n",
    "print(\"Starting Positive samples...\")\n",
    "start_time = time.time()\n",
    "pos_data = list(map(lambda x: build_feature_vector(x), pos_matches))\n",
    "for vector in pos_data:\n",
    "    vector.append(1)\n",
    "print(\"Number of Positive samples: {}\".format(len(pos_data)))\n",
    "print(\"Time to finish positive samples: {}\".format(time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Starting Negative samples...\")\n",
    "neg_data = list(map(lambda x: build_feature_vector(x), neg_matches))\n",
    "for vector in neg_data:\n",
    "    vector.append(0)\n",
    "print(\"Number of Negative samples: {}\".format(len(neg_data)))\n",
    "print(\"Time to finish negative samples: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature vectors for potential ceo samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting classification of potential samples...\n",
      "Number of samples: 189502\n",
      "Time to finish: 540.3816339969635\n"
     ]
    }
   ],
   "source": [
    "# Add Labels\n",
    "print(\"Starting classification of potential samples...\")\n",
    "start_time = time.time()\n",
    "new_data = list(map(lambda x: build_feature_vector(x), potential_matches))\n",
    "print(\"Number of samples: {}\".format(len(new_data)))\n",
    "print(\"Time to finish: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model, Random Forest\n",
    "\n",
    "Split the given data into training and testing set, 2/3 - 1/3 split. We then train a random forest and tune parameters using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128071    280]\n",
      " [   261  25756]]\n",
      "Accuracy: 0.9964953876451078\n",
      "Precision: 0.9978184821310313\n",
      "Recall: 0.9979662126359754\n"
     ]
    }
   ],
   "source": [
    "# combine the data\n",
    "combined_data = pd.DataFrame(pos_data + neg_data)\n",
    "x_values = combined_data.drop(15, axis = 1)\n",
    "y_values = combined_data[15]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values, y_values, test_size=0.33)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 40)\n",
    "rfc.fit(x_train, y_train)\n",
    "rf_fit_values = rfc.predict(x_train)\n",
    "confus = confusion_matrix(y_train, rf_fit_values)\n",
    "print(confus)\n",
    "accuracy = (confus[0,0] + confus[1,1]) / sum(sum(confus))\n",
    "precision = (confus[0,0] / (confus[0,0] + confus[0,1]))\n",
    "recall = (confus[0,0] / (confus[0,0] + confus[1,0]))\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[61687  1623]\n",
      " [ 4793  7929]]\n",
      "Accuracy: 0.9156144781144782\n",
      "Precision: 0.974364239456642\n",
      "Recall: 0.9279031287605295\n"
     ]
    }
   ],
   "source": [
    "rf_fit_values = rfc.predict(x_test)\n",
    "confus = confusion_matrix(y_test, rf_fit_values)\n",
    "print(confus)\n",
    "accuracy = (confus[0,0] + confus[1,1]) / sum(sum(confus))\n",
    "precision = (confus[0,0] / (confus[0,0] + confus[0,1]))\n",
    "recall = (confus[0,0] / (confus[0,0] + confus[1,0]))\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on the samples pulled from the corpus, classify and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit_values = rfc.predict(new_data)\n",
    "classified_ceos = []\n",
    "for index, classification in enumerate(rf_fit_values):\n",
    "    if classification == 1:\n",
    "        classified_ceos.append(potential_matches[index][0])\n",
    "found_ceos = set(classified_ceos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(set(list(found_ceos) + cleaned_ceos))\n",
    "pd.Series(output).to_csv(\"found_ceos.csv\", index = False, header = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
